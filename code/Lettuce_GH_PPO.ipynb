{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from environment import LettuceGreenhouse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import yaml\n",
    "import datetime\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the simulation....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir) \n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "        \n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose >= 1:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose >= 1:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "        \n",
    "\n",
    "        info = self.locals['infos'][0]\n",
    "        plt.plot(info['timestep_plot'], info['supply_co2_plot'])\n",
    "        plt.xlabel('Time in 15 min steps')\n",
    "        plt.ylabel('Supply rate of carbon dioxide [mg]/[m^2][s]')\n",
    "        plot_co2_supply = plt.gcf()\n",
    "        plt.close()\n",
    "        ### get the \n",
    "        plt.plot(info['timestep_plot'], info['indoor_co2_plot'])\n",
    "        plt.xlabel('Time in 15 min steps')\n",
    "        plt.ylabel('Indoor COÂ¬2 concentration [ppm]')\n",
    "        plot_co2_indoor = plt.gcf()\n",
    "        plt.close()\n",
    "        # # Generate the plot\n",
    "        self.writer.add_figure('Supply Rate of CO2', plot_co2_supply, self.num_timesteps)  # Log the plot to TensorBoard\n",
    "        self.writer.add_figure('Indoor CO2', plot_co2_indoor, self.num_timesteps)  # Log the plot to TensorBoard\n",
    "\n",
    "        return True\n",
    "class EvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for evaluating an agent.\n",
    "\n",
    "    :param eval_env: (gym.Env) The environment used for initialization\n",
    "    :param n_eval_episodes: (int) The number of episodes to test the agent\n",
    "    :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, n_eval_episodes=5, eval_freq=20):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self):\n",
    "        \"\"\"\n",
    "        This method will be called by the model.\n",
    "\n",
    "        :return: (bool)\n",
    "        \"\"\"\n",
    "\n",
    "        # self.n_calls is automatically updated because\n",
    "        # we derive from BaseCallback\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            # === YOUR CODE HERE ===#\n",
    "            # Evaluate the agent:\n",
    "            # you need to do self.n_eval_episodes loop using self.eval_env\n",
    "            # hint: you can use self.model.predict(obs, deterministic=True)\n",
    "\n",
    "            # Save the agent if needed\n",
    "            # and update self.best_mean_reward\n",
    "\n",
    "            print(\"Best mean reward: {:.2f}\".format(self.best_mean_reward))\n",
    "\n",
    "            # ====================== #\n",
    "        return True\n",
    "# Create log and model directories dir\n",
    "log_dir = f\"logs/{int(time.time())}/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "os.makedirs(models_dir)\n",
    "\n",
    "\n",
    "# Create the callback: check every 1000 steps\n",
    "## check every 1000 steps to see if ideal model has been found....\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above we see our class that will essentially look for the best mean reward and then save that model..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next as we see below we will initialize our environment. This in our case is the greenhouse environment. We will also reset the greenhouse environment to initialze it.\n",
    "\n",
    "Then the model will be defined based on a multi perceptron policy, and then we initialize the callback function for what will happen while the model is learning so while we check for best model every 1000 steps, we are running our model for 10000 timesteps.... \n",
    "-- this timesteps means that we will finish the environment time steps and then we will continue on until 10000 is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# create the environment object for the greenhouse environment\n",
    "gh =  LettuceGreenhouse()\n",
    "# initialize the greenhouse environment\n",
    "gh.reset()\n",
    "\n",
    "# Logs will be saved in log_dir/monitor.csv\n",
    "## Used to know episode reward, length, time, and other data...\n",
    "env = Monitor(gh,log_dir)\n",
    "\n",
    "#seed for reproducability\n",
    "seed = 5\n",
    "\n",
    "# Number of Timesteps\n",
    "n_steps=  10000\n",
    "\n",
    "# Train a the PPO agent\n",
    "## Set the seed so it is repeatable\n",
    "model = PPO(\"MlpPolicy\", env, seed=seed, verbose=1)\n",
    "\n",
    "for i in range(1,3):\n",
    "    # Pass the callback object to the model's `learn()` method\n",
    "    model.learn(total_timesteps=n_steps, reset_num_timesteps=False, callback = callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# results_plotter.plot_results([log_dir],1e5, results_plotter.X_TIMESTEPS, \"PPO Greenhouse Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mean_reward, std_reward \u001b[39m=\u001b[39m evaluate_policy(model, model\u001b[39m.\u001b[39;49mget_env(), n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe mean reward is \u001b[39m\u001b[39m{\u001b[39;00mmean_reward\u001b[39m}\u001b[39;00m\u001b[39m and the standard deviation of the reward is \u001b[39m\u001b[39m{\u001b[39;00mstd_reward\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\lkrow\\stable-baselines3\\stable_baselines3\\common\\evaluation.py:87\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m     86\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(observations, state\u001b[39m=\u001b[39mstates, episode_start\u001b[39m=\u001b[39mepisode_starts, deterministic\u001b[39m=\u001b[39mdeterministic)\n\u001b[1;32m---> 87\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m     88\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m     89\u001b[0m     current_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\lkrow\\stable-baselines3\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\users\\lkrow\\stable-baselines3\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\users\\lkrow\\stable-baselines3\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\lkrow\\School\\WUR\\Period 6\\Adv_ML\\Project\\Lettuce-Greenhouse-Production-Control\\code\\environment.py:192\u001b[0m, in \u001b[0;36mLettuceGreenhouse.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mold_state \u001b[39m=\u001b[39m obs\n\u001b[0;32m    191\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m--> 192\u001b[0m     obs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimestep\u001b[39m+\u001b[39;49mi]))\n\u001b[0;32m    193\u001b[0m observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(obs , dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    195\u001b[0m \u001b[39m# observation  = np.zeros((84,))\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# observation[:4] = [obs[0],obs[1],obs[2],obs[3]]\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m# observation = np.array(observation , dtype=np.float32)\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[39m# Now we will add our data that we will be plotting to info\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39m## info is a dictionary that is able to be accessed locally...\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=2)\n",
    "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
       "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
       "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
       "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
       "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
       "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
       "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
       "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
       "                   [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--detect_file_replacement BOOL]\n",
       "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
       "                   [--whatif-data-dir PATH]\n",
       "                   {serve,dev} ...\n",
       "tensorboard: error: argument {serve,dev}: invalid choice: 'GH-PPO-run_1)' (choose from 'serve', 'dev')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ppo_model_name = f'PPO-run'\n",
    "folder =  ppo_model_name + \"_1\"\n",
    "log_dir_tensorboard = os.path.join(log_dir, folder)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $log_dir_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST_Lettuce_GH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
